{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this notbeook I do some durther preprocessing and data engineering. There are still questions I want to answer about the final data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import isnan, when, count, col, udf, year, month, dayofmonth, dayofweek, datediff, to_date, regexp_replace, length, unix_timestamp, from_unixtime, log\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from constants import LOGISTICS_DATA, MATERIALS_DATA, PROJECTS_DATA, SUPPLIERS_DATA, DATA, FINAL_DATA_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/11/05 01:56:51 WARN Utils: Your hostname, Kris resolves to a loopback address: 127.0.1.1; using 172.18.209.221 instead (on interface eth0)\n",
      "23/11/05 01:56:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/05 01:56:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CO2 Emission ML Pipeline - Data Preprocessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#reading df\n",
    "final_df = spark.read.parquet(FINAL_DATA_parquet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Date Columns\n",
    "I noticed that Transaction_Date column is in string format. Upon lookin ginto it I notice that some dates are in the wrong format. Here, I'm fixing this issue and then converting the column to_date to continue my preprocessing.\n",
    "\n",
    "There are 9175 dates with a format that is not recognized as dd/MM/yyyy, and 1025 dates are in the right dd/MM/yyyy format. The date parsing function is expecting a specific format and cannot parse the dates that do not conform to it.\n",
    "\n",
    "..this step was a bit annoying and took longer than I expcted :@ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|transaction_date|\n",
      "+----------------+\n",
      "|2020/01/01      |\n",
      "|2020/01/01      |\n",
      "|2020/01/01      |\n",
      "|2020/01/01      |\n",
      "|2020/01/01      |\n",
      "|2020/01/01      |\n",
      "|2020/01/01      |\n",
      "|2020/01/01      |\n",
      "|2020/01/01      |\n",
      "|2020/01/01      |\n",
      "+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace hyphens with slashes in the transaction_date column\n",
    "final_df = final_df.withColumn(\"transaction_date\", regexp_replace(\"transaction_date\", \"-\", \"/\"))\n",
    "\n",
    "# Show the updated DataFrame to confirm the changes\n",
    "final_df.select(\"transaction_date\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|transaction_date|\n",
      "+----------------+\n",
      "|2020-01-01      |\n",
      "|2020-01-01      |\n",
      "|2020-01-01      |\n",
      "|2020-01-01      |\n",
      "|2020-01-01      |\n",
      "|2020-01-01      |\n",
      "|2020-01-01      |\n",
      "|2020-01-01      |\n",
      "|2020-01-01      |\n",
      "|2020-01-01      |\n",
      "+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a function to convert different formats to date type\n",
    "def convert_date_format(df, col_name):\n",
    "    return df.withColumn(\n",
    "        col_name, \n",
    "        when(\n",
    "            col(col_name).rlike(\"^\\d{4}/\\d{2}/\\d{2}$\"),  # matches date like 'yyyy/MM/dd'\n",
    "            to_date(col(col_name), 'yyyy/MM/dd')\n",
    "        ).otherwise(\n",
    "            to_date(col(col_name), 'dd/MM/yyyy')  # assumes the date is in 'dd/MM/yyyy' if not 'yyyy/MM/dd'\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Apply the function to your dataframe\n",
    "final_df = convert_date_format(final_df, \"transaction_date\")\n",
    "\n",
    "# Show the results\n",
    "final_df.select(\"transaction_date\").show(10, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+----------------+\n",
      "|Transaction_Date|project_start_date|project_end_date|\n",
      "+----------------+------------------+----------------+\n",
      "|               0|                 0|               0|\n",
      "+----------------+------------------+----------------+\n",
      "\n",
      "[('material_id', 'int'), ('project_id', 'int'), ('project_budget', 'double'), ('transaction_id', 'int'), ('transaction_date', 'date'), ('quantity', 'int'), ('transport_mode', 'string'), ('distance_covered', 'double'), ('CO2_emission', 'double'), ('project_name', 'string'), ('project_start_date', 'date'), ('project_end_date', 'date'), ('project_location', 'string'), ('supplier_id', 'int'), ('material_name', 'string'), ('material_category', 'string'), ('supplier_name', 'string'), ('supplier_location', 'string'), ('supplier_rating', 'double')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check for nulls after conversion\n",
    "null_counts = final_df.select([count(when(col(c).isNull(), c)).alias(c) for c in [\"Transaction_Date\", \"project_start_date\", \"project_end_date\"]])\n",
    "null_counts.show()\n",
    "print(final_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Year, Month, and Day from 'Transaction_Date'\n",
    "final_df = final_df.withColumn(\"Transaction_Year\", year(\"transaction_date\"))\n",
    "final_df = final_df.withColumn(\"Transaction_Month\", month(\"transaction_date\"))\n",
    "final_df = final_df.withColumn(\"Transaction_Day\", dayofmonth(\"transaction_date\"))\n",
    "final_df = final_df.withColumn(\"is_weekend\", (dayofweek(\"transaction_date\").isin([1, 7])).cast(\"int\"))\n",
    "final_df = final_df.withColumn(\"project_duration\", datediff(\"project_end_date\", \"project_start_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA SUMMARY\n",
    "\n",
    "Based on the summary statistics results below, the project_budget and CO2_emission variables stand out as the maximum value is substantially larger than the mean, which suggests a long right tail, indicative of a skewed distribution. Usually in such cases, a log transformation helps in stabilizing the variance across levels of input variables and making the relationships more linear for the predictive modeling.\n",
    "- CO2_emission: As the target variable, it's essential that the error terms in any model predicting it are normally distributed. If CO2_emission is highly skewed, a log transformation could help to achieve a more normally distributed error term.\n",
    "- project_budget: The maximum value is substantially larger than the mean, which suggests a long right tail, indicative of a skewed distribution. A log transformation could normalize this, which would be particularly useful if project_budget is a feature in a model predicting CO2_emissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/05 01:57:08 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for material_id:\n",
      "Mean: 496.76872549019606\n",
      "Max: 1000\n",
      "Min: 1\n",
      "Standard Deviation: 290.96310745073055\n",
      "Median: 497.0\n",
      "--------------\n",
      "Statistics for project_id:\n",
      "Mean: 50.42980392156863\n",
      "Max: 100\n",
      "Min: 1\n",
      "Standard Deviation: 28.75723049387397\n",
      "Median: 51.0\n",
      "--------------\n",
      "Statistics for project_budget:\n",
      "Mean: 295840.8306163639\n",
      "Max: 1950581.5483643755\n",
      "Min: 53805.64013990858\n",
      "Standard Deviation: 209570.71331281099\n",
      "Median: 270037.4610087137\n",
      "--------------\n",
      "Statistics for transaction_id:\n",
      "Mean: 5006.130980392157\n",
      "Max: 10000\n",
      "Min: 1\n",
      "Standard Deviation: 2884.331836773124\n",
      "Median: 5009.0\n",
      "--------------\n",
      "Statistics for quantity:\n",
      "Mean: 50.396470588235296\n",
      "Max: 99\n",
      "Min: 1\n",
      "Standard Deviation: 28.470351337471453\n",
      "Median: 50.0\n",
      "--------------\n",
      "Statistics for transport_mode:\n",
      "Mean: None\n",
      "Max: Truck\n",
      "Min: Drone\n",
      "Standard Deviation: None\n",
      "Median: N/A\n",
      "--------------\n",
      "Statistics for distance_covered:\n",
      "Mean: 304.99896981195866\n",
      "Max: 4989.07372254418\n",
      "Min: 10.00929808408966\n",
      "Standard Deviation: 409.6849422645481\n",
      "Median: 266.75006573668037\n",
      "--------------\n",
      "Statistics for CO2_emission:\n",
      "Mean: 3878.3788220057672\n",
      "Max: 22261.41573065918\n",
      "Min: 3.160525025280295\n",
      "Standard Deviation: 3808.2366286280417\n",
      "Median: 2677.8490194696\n",
      "--------------\n",
      "Statistics for project_name:\n",
      "Mean: None\n",
      "Max: project_84\n",
      "Min: Project_1\n",
      "Standard Deviation: None\n",
      "Median: N/A\n",
      "--------------\n",
      "Statistics for project_location:\n",
      "Mean: None\n",
      "Max: City_D\n",
      "Min: City_A\n",
      "Standard Deviation: None\n",
      "Median: N/A\n",
      "--------------\n",
      "Statistics for supplier_id:\n",
      "Mean: 5.350294117647059\n",
      "Max: 10\n",
      "Min: 1\n",
      "Standard Deviation: 2.8873241523300095\n",
      "Median: 5.0\n",
      "--------------\n",
      "Statistics for material_name:\n",
      "Mean: None\n",
      "Max: Material_999\n",
      "Min: Material_1\n",
      "Standard Deviation: None\n",
      "Median: N/A\n",
      "--------------\n",
      "Statistics for material_category:\n",
      "Mean: None\n",
      "Max: Structural\n",
      "Min: Binder\n",
      "Standard Deviation: None\n",
      "Median: N/A\n",
      "--------------\n",
      "Statistics for supplier_name:\n",
      "Mean: None\n",
      "Max: Supplier_9\n",
      "Min: Supplier_1\n",
      "Standard Deviation: None\n",
      "Median: N/A\n",
      "--------------\n",
      "Statistics for supplier_location:\n",
      "Mean: None\n",
      "Max: City_G\n",
      "Min: City_E\n",
      "Standard Deviation: None\n",
      "Median: N/A\n",
      "--------------\n",
      "Statistics for supplier_rating:\n",
      "Mean: 3.364313725490196\n",
      "Max: 5.0\n",
      "Min: 1.0\n",
      "Standard Deviation: 1.5019747172923015\n",
      "Median: 4.0\n",
      "--------------\n",
      "Statistics for Transaction_Year:\n",
      "Mean: 2020.9046078431372\n",
      "Max: 2022\n",
      "Min: 2020\n",
      "Standard Deviation: 0.7903094467420779\n",
      "Median: 2021.0\n",
      "--------------\n",
      "Statistics for Transaction_Month:\n",
      "Mean: 6.099509803921569\n",
      "Max: 12\n",
      "Min: 1\n",
      "Standard Deviation: 3.305772219698186\n",
      "Median: 6.0\n",
      "--------------\n",
      "Statistics for Transaction_Day:\n",
      "Mean: 15.66421568627451\n",
      "Max: 31\n",
      "Min: 1\n",
      "Standard Deviation: 8.780537951786656\n",
      "Median: 16.0\n",
      "--------------\n",
      "Statistics for is_weekend:\n",
      "Mean: 0.28519607843137257\n",
      "Max: 1\n",
      "Min: 0\n",
      "Standard Deviation: 0.45152991422697764\n",
      "Median: 0.0\n",
      "--------------\n",
      "Statistics for project_duration:\n",
      "Mean: 182.07519607843136\n",
      "Max: 184\n",
      "Min: 181\n",
      "Standard Deviation: 1.0334784537039596\n",
      "Median: 182.0\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "# Calculate summary statistics\n",
    "summary_stats = final_df.describe()\n",
    "\n",
    "# Fetch the statistics for each column from the summary DataFrame\n",
    "means = {row['summary']: row.asDict() for row in summary_stats.collect()}[\"mean\"]\n",
    "maxs = {row['summary']: row.asDict() for row in summary_stats.collect()}[\"max\"]\n",
    "mins = {row['summary']: row.asDict() for row in summary_stats.collect()}[\"min\"]\n",
    "stddevs = {row['summary']: row.asDict() for row in summary_stats.collect()}[\"stddev\"]\n",
    "\n",
    "# calculate median for each numerical column\n",
    "medians = {}\n",
    "for column in final_df.columns:\n",
    "    # Check if column is numerical by trying to cast it to a double; skip if casting fails (essentially changing the var type)\n",
    "    try:\n",
    "        final_df_numerical = final_df.withColumn(column, final_df[column].cast('double'))\n",
    "        medians[column] = final_df_numerical.approxQuantile(column, [0.5], 0.0)[0]\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "# Display stats for each column\n",
    "for column in final_df_numerical.columns:\n",
    "    if column in means:  # This checks if the column is numerical (present in the summary stats)\n",
    "        print(f\"Statistics for {column}:\")\n",
    "        print(f\"Mean: {means[column]}\")\n",
    "        print(f\"Max: {maxs[column]}\")\n",
    "        print(f\"Min: {mins[column]}\")\n",
    "        print(f\"Standard Deviation: {stddevs[column]}\")\n",
    "        print(f\"Median: {medians.get(column, 'N/A')}\")\n",
    "        print(\"--------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Transformation\n",
    "\n",
    "Doing Log transform of project_budget and CO2_emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding log-transformed columns for project_budget and CO2_emission\n",
    "final_df = final_df.withColumn(\"log_project_budget\", log(\"project_budget\"))\n",
    "final_df = final_df.withColumn(\"log_CO2_emission\", log(\"CO2_emission\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CORRELATION BETWEEN NUMERICAL FEATURES AND TARGET FEATURE CO2_EMISSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlation between CO2_emissions and material_id is: 0.013309843960719402\n",
      "The correlation between CO2_emissions and project_id is: 0.015389057139567884\n",
      "The correlation between CO2_emissions and project_budget is: -0.008704442492230928\n",
      "The correlation between CO2_emissions and transaction_id is: 0.011334588931078056\n",
      "The correlation between CO2_emissions and quantity is: 0.5558180227071717\n",
      "The correlation between CO2_emissions and distance_covered is: 0.21013827019533382\n",
      "The correlation between CO2_emissions and supplier_id is: -0.01578169754900137\n",
      "The correlation between CO2_emissions and supplier_rating is: 0.00353117673371521\n",
      "The correlation between CO2_emissions and Transaction_Year is: 0.011702046301184451\n",
      "The correlation between CO2_emissions and Transaction_Month is: -0.0010327637222996807\n",
      "The correlation between CO2_emissions and Transaction_Day is: 4.095459896112875e-06\n",
      "The correlation between CO2_emissions and is_weekend is: -0.014622640639648167\n",
      "The correlation between CO2_emissions and project_duration is: -0.021658391461595122\n",
      "The correlation between CO2_emissions and log_project_budget is: -0.012067149901191152\n",
      "The correlation between CO2_emissions and log_CO2_emission is: 0.8078794093947689\n"
     ]
    }
   ],
   "source": [
    "# List of features\n",
    "features = final_df.columns\n",
    "features.remove('CO2_emission')  # Remove the target variable\n",
    "\n",
    "# Calculate correlations with the target variable\n",
    "correlations = [(feature, final_df.stat.corr(feature, 'CO2_emission')) for feature in features if final_df.select(feature).dtypes[0][1] in ('double', 'int')]\n",
    "\n",
    "for feature in correlations:\n",
    "    print(f\"The correlation between CO2_emissions and {feature[0]} is: {feature[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that \"Quantity\" and \"Distance Covered\" are somewhat correlated with the target variable, I can create polynomial features to capture non-linear relationship. An interaction term can capture the combined effect of two variables. For example, the interaction between \"Quantity\" and \"Distance Covered\" might be informative.\n",
    "\n",
    "Creating polynomial features can be a useful step in capturing non-linear relationships between the features and the target variable. In a linear model, each feature is multiplied by a weight and summed up to make a prediction. This assumes that the relationship between each feature and the target variable is linear, but the real-world relationship between variables can often be more complex and non-linear.\n",
    "\n",
    "For example, the relationship between \"Quantity\" and \"CO2_emission\" might not be a straight line; it could be a curve. In such cases, simply using the \"Quantity\" feature as-is in a linear model might not capture this curve effectively. But if you add a new feature that is \"Quantity\" squared, the model has a better chance of capturing this curved relationship.\n",
    "\n",
    "The same logic applies to \"Distance Covered,\" or any interaction terms between \"Quantity\" and \"Distance Covered.\" By including these polynomial and interaction terms, you allow the model to fit to a more flexible, potentially non-linear function, which could result in a more accurate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create polynomial features for 'Quantity' and 'Distance Covered'\n",
    "final_df = final_df.withColumn(\"Quantity_Squared\", col(\"Quantity\")**2)\n",
    "final_df = final_df.withColumn(\"Distance_Covered_Squared\", col(\"Distance_Covered\")**2)\n",
    "\n",
    "# Create interaction term between 'Quantity' and 'Distance Covered'\n",
    "final_df = final_df.withColumn(\"Quantity_Distance_Interaction\", col(\"Quantity\") * col(\"Distance_Covered\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN/TEST SPLIT - ONE HOT ENCODING AND SCALING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "train_df, test_df = final_df.randomSplit([0.80, 0.20], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "identifiers dont need to be scaled (material_id, project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical columns to one hot encode\n",
    "categorical_columns = [\"transport_mode\", \"project_location\", \"material_category\", \"supplier_location\"]\n",
    "# list of columns to scale\n",
    "columns_to_scale = [\"Quantity_Squared\", \"Distance_Covered_Squared\", \"Quantity_Distance_Interaction\", \"supplier_rating\", \"Transaction_Year\", \"Transaction_Month\", \"project_duration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Stages in the pipeline\n",
    "stages = []\n",
    "\n",
    "# One-Hot Encoding for categorical columns\n",
    "for categoricalCol in categorical_columns:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"Vec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "# Assemble numerical columns that need to be scaled\n",
    "assembler_for_scaling = VectorAssembler(inputCols=columns_to_scale, outputCol=\"features_to_scale\")\n",
    "stages += [assembler_for_scaling]\n",
    "\n",
    "# Scale the numerical columns\n",
    "scaler = StandardScaler(inputCol=\"features_to_scale\", outputCol=\"scaledFeatures\")\n",
    "stages += [scaler]\n",
    "\n",
    "# Assemble all features into one vector column\n",
    "assembledInputs = [c + \"Vec\" for c in categorical_columns] + [\"scaledFeatures\"] + [\"log_project_budget\"]\n",
    "final_assembler = VectorAssembler(inputCols=assembledInputs, outputCol=\"features\")\n",
    "stages += [final_assembler]\n",
    "\n",
    "# Create a Pipeline with the stages defined above\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "# Transform the training and test data\n",
    "train_transformed = pipeline_model.transform(train_df) # use this to train model\n",
    "test_transformed = pipeline_model.transform(test_df) # use this to test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.5787417089628518\n",
      "R-squared on test data = 0.796317652770236\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize the RandomForest regressor\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"log_CO2_emission\")\n",
    "\n",
    "# Train the model\n",
    "rf_model = rf.fit(train_transformed)\n",
    "\n",
    "# Predict on the test data\n",
    "predictions = rf_model.transform(test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"log_CO2_emission\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")\n",
    "\n",
    "# If you want to evaluate R-squared\n",
    "evaluator = RegressionEvaluator(labelCol=\"log_CO2_emission\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "print(f\"R-squared on test data = {r2}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Length of feature names (18) does not match the number of importances (17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/krisko/ML-pipeline-Co2/src/data_preprocessing.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/krisko/ML-pipeline-Co2/src/data_preprocessing.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m feature_names\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39mlog_project_budget\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/krisko/ML-pipeline-Co2/src/data_preprocessing.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# The length of feature names should now match the length of importances\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/krisko/ML-pipeline-Co2/src/data_preprocessing.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(feature_names) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(importances), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLength of feature names (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(feature_names)\u001b[39m}\u001b[39;00m\u001b[39m) does not match the number of importances (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(importances)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/krisko/ML-pipeline-Co2/src/data_preprocessing.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Now you can match the importances to the feature names\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/krisko/ML-pipeline-Co2/src/data_preprocessing.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m named_importances \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(\u001b[39mzip\u001b[39m(feature_names, importances), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m1\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Length of feature names (18) does not match the number of importances (17)"
     ]
    }
   ],
   "source": [
    "# Get the feature importances\n",
    "importances = rf_model.featureImportances.toArray()\n",
    "\n",
    "# Start with an empty list for feature names\n",
    "feature_names = []\n",
    "\n",
    "# Add names for the one-hot encoded categorical features\n",
    "# You need to know the number of categories in each categorical feature after one-hot encoding\n",
    "for categoricalCol in categorical_columns:\n",
    "    # Assuming we know the number of categories for each column (replace with actual number)\n",
    "    num_categories = train_transformed.select(categoricalCol + \"Vec\").head()[0].size\n",
    "    feature_names += [f\"{categoricalCol}_{i}\" for i in range(num_categories)]\n",
    "\n",
    "# Add names for the scaled numerical features\n",
    "feature_names += columns_to_scale\n",
    "\n",
    "# Add the log-transformed features if they are also included\n",
    "feature_names.append(\"log_project_budget\")\n",
    "\n",
    "# The length of feature names should now match the length of importances\n",
    "assert len(feature_names) == len(importances), f\"Length of feature names ({len(feature_names)}) does not match the number of importances ({len(importances)})\"\n",
    "\n",
    "# Now you can match the importances to the feature names\n",
    "named_importances = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the feature importances\n",
    "for name, importance in named_importances:\n",
    "    print(f\"{name}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLops-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
