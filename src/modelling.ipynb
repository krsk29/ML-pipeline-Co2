{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml.regression import RandomForestRegressor, LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "from constants import TEST_TRANSFORMED_DATA, TRAIN_TRANSFORMED_DATA, MODELS, RANDOM_FOREST_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/11/18 00:12:01 WARN Utils: Your hostname, Kris resolves to a loopback address: 127.0.1.1; using 172.18.209.221 instead (on interface eth0)\n",
      "23/11/18 00:12:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/18 00:12:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"CO2 Emission ML Pipeline - Modelling\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#reading df\n",
    "TEST_TRANSFORMED_DF = spark.read.parquet(TEST_TRANSFORMED_DATA)\n",
    "TRAIN_TRANSFORMED_DF = spark.read.parquet(TRAIN_TRANSFORMED_DATA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical columns \n",
    "categorical_columns = [\"transport_mode\", \"project_location\", \"material_category\", \"supplier_location\"]\n",
    "# list of scaled columns\n",
    "columns_to_scale = [\"Quantity_Squared\", \"Distance_Covered_Squared\", \"Quantity_Distance_Interaction\", \"supplier_rating\", \"Transaction_Year\", \"Transaction_Month\", \"project_duration\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 00:12:17 WARN Instrumentation: [cdc5c87c] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/11/18 00:12:19 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/11/18 00:12:19 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 1.0529085351593626\n",
      "R-squared on test data = 0.3258366813389463\n"
     ]
    }
   ],
   "source": [
    "# initialize the Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"log_CO2_emission\")\n",
    "\n",
    "# train model\n",
    "lr_model = lr.fit(TRAIN_TRANSFORMED_DF)\n",
    "\n",
    "# predict on the test data\n",
    "lr_predictions = lr_model.transform(TEST_TRANSFORMED_DF)\n",
    "\n",
    "# evaluate the model\n",
    "lr_evaluator = RegressionEvaluator(labelCol=\"log_CO2_emission\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "lr_rmse = lr_evaluator.evaluate(lr_predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {lr_rmse}\")\n",
    "\n",
    "lr_evaluator = RegressionEvaluator(labelCol=\"log_CO2_emission\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "lr_r2 = lr_evaluator.evaluate(lr_predictions)\n",
    "print(f\"R-squared on test data = {lr_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.026265564908581642,-0.023614727314001275,0.014240011875558171,-0.009430261892876688,-0.03693966286356247,0.0008391584442805165,-0.007710394896382353,-0.01580567302404461,0.005050603664929159,0.43910718992227366,-0.42599632305299817,0.6839255190338015,-0.003597240585000929,-0.014830168210552427,-0.0106627065498407,-0.009004012399024564,0.0011711854383697421]\n"
     ]
    }
   ],
   "source": [
    "# coefficients represent the change in the response variable for a one-unit change in the respective feature, assuming all other features remain constant.\n",
    "# coefficients are sort of an indicator for feature importance\n",
    "print(lr_model.coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression model is underftting. It is not capturing the data patterns; with an r2 of 32% it means that much of the variance in the target variable is not captured by the model. Also, high RMSE indicates poor odel performance.\n",
    "In such cases, it's useful to revisit the data as a whole and its features. Regularisation here is pointless because the model is weak and not complex enough for regularisation to help in any way. In fact, regularisation is used to prevent overfitting.\n",
    "\n",
    "Regularisation L1 (Lasso): helps with feature selection and effectively removes certain features. Likely that only subset of features is important. It adds a penalty equal to the absolute value of the magnitude of the model coefficients.\n",
    "\n",
    "Regularisation L2 (Ridge Regression): this adds a penalty equal to the square of the magnitude of coefficients. Unlike L1 it does not reduce coeffiecnts to zero but it minimises their impact. This is usefult for overfitting models and in cases of multicollinearity.\n",
    "\n",
    "The penalties are added to the loss function during training. The loss function is used to quanitfy how well the model is performing in terms of making predictions compared to the actual data. It is the difference between the model's prediciton and the actual data. The goal is to minimise this loff during model's training. There are different types of loss functions depending on what problem we dealing with. MSE for regression or Binary Cross entropy for binary classification for example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize GBTRegressor\n",
    "# gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"log_CO2_emission\")\n",
    "\n",
    "# # create parameter grid \n",
    "# gbt_paramGrid = (ParamGridBuilder()\n",
    "#                  .addGrid(gbt.maxDepth, [5, 10])\n",
    "#                  .addGrid(gbt.maxBins, [16])\n",
    "#                  .addGrid(gbt.maxIter, [10])\n",
    "#                  .build())\n",
    "\n",
    "# # initialize evaluator with the appropriate metric - rmse\n",
    "# gbt_evaluator = RegressionEvaluator(\n",
    "#     labelCol=\"log_CO2_emission\", predictionCol=\"prediction\", metricName=\"rmse\"\n",
    "# )\n",
    "\n",
    "# # 5-fold CrossValidator\n",
    "# gbt_cv = CrossValidator(estimator=gbt,\n",
    "#                         estimatorParamMaps=gbt_paramGrid,\n",
    "#                         evaluator=gbt_evaluator,\n",
    "#                         numFolds=3)\n",
    "\n",
    "# # run cross-validation\n",
    "# gbt_cv_model = gbt_cv.fit(TRAIN_TRANSFORMED_DF)\n",
    "\n",
    "# # predict on the test data\n",
    "# gbt_cv_predictions = gbt_cv_model.transform(TEST_TRANSFORMED_DF)\n",
    "\n",
    "# # evaluate the model\n",
    "# gbt_cv_rmse = gbt_evaluator.evaluate(gbt_cv_predictions)\n",
    "# print(f\"Root Mean Squared Error (RMSE) on test data with CV = {gbt_cv_rmse}\")\n",
    "\n",
    "# # To evaluate R-squared\n",
    "# gbt_evaluator.setMetricName(\"r2\")\n",
    "# gbt_cv_r2 = gbt_evaluator.evaluate(gbt_cv_predictions)\n",
    "# print(f\"R-squared on test data with CV = {gbt_cv_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/14 17:47:14 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "WARNING: An illegal reflective access operation has occurred                    \n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/home/krisko/ML-pipeline-Co2/venv_co2/lib/python3.10/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/11/14 17:47:25 WARN DAGScheduler: Broadcasting large task binary with size 1447.3 KiB\n",
      "23/11/14 17:47:31 WARN DAGScheduler: Broadcasting large task binary with size 1313.1 KiB\n",
      "23/11/14 17:47:32 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/11/14 17:47:34 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "23/11/14 17:47:42 WARN DAGScheduler: Broadcasting large task binary with size 1370.9 KiB\n",
      "23/11/14 17:47:47 WARN DAGScheduler: Broadcasting large task binary with size 1305.3 KiB\n",
      "23/11/14 17:47:48 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/11/14 17:47:49 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "23/11/14 17:47:56 WARN DAGScheduler: Broadcasting large task binary with size 1408.0 KiB\n",
      "23/11/14 17:48:01 WARN DAGScheduler: Broadcasting large task binary with size 1294.5 KiB\n",
      "23/11/14 17:48:02 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/11/14 17:48:03 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "23/11/14 17:48:08 WARN DAGScheduler: Broadcasting large task binary with size 1231.6 KiB\n",
      "23/11/14 17:48:10 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/11/14 17:48:12 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on CV test data = 0.5571811882243928\n",
      "R-squared on CV test data = 0.8112109866661488\n"
     ]
    }
   ],
   "source": [
    "# initialize RandomForest regressor\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"log_CO2_emission\")\n",
    "\n",
    "# parameter grid \n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.numTrees, [10, 30])  # List of trees to test\n",
    "             .addGrid(rf.maxDepth, [5, 10])    # List of maximum depths to test\n",
    "             .addGrid(rf.maxBins, [32])        # List of bins to test\n",
    "             .build())\n",
    "\n",
    "# evaluator for the cross-validation\n",
    "evaluator = RegressionEvaluator(labelCol=\"log_CO2_emission\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# crossValidator requires the same evaluator used to evaluate the model\n",
    "cv = CrossValidator(estimator=rf,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=3)  # Number of folds for cross-validation\n",
    "\n",
    "# run cross-validation\n",
    "cv_model = cv.fit(TRAIN_TRANSFORMED_DF)\n",
    "\n",
    "# Use the best model found to make predictions on the test data\n",
    "cv_predictions = cv_model.transform(TEST_TRANSFORMED_DF)\n",
    "\n",
    "# evaluate best model\n",
    "cv_rmse = evaluator.evaluate(cv_predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on CV test data = {cv_rmse}\")\n",
    "\n",
    "#R-squared eevaluation\n",
    "evaluator.setMetricName(\"r2\")\n",
    "cv_r2 = evaluator.evaluate(cv_predictions)\n",
    "print(f\"R-squared on CV test data = {cv_r2}\")\n",
    "\n",
    "# Get best model\n",
    "best_rf_model = cv_model.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Outcome\n",
    "The results show that both the Gradient Boosted Trees (GBT) and Random Forest Regressor models have performed significantly better than the Linear Regression model in terms of both RMSE and R-squared. The RMSE is lower for the GBT and Random Forest models, indicating better accuracy, and the R-squared values are significantly higher, suggesting that these models explain a much greater proportion of the variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance Analysis\n",
    "Analyzing feature importance is a crucial step in understanding and interpreting the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Feature Importance\n",
    "I'm taking Random Forest Regressor as my model of choice.\n",
    "\n",
    "With PySpark we can use the attribute 'featureImportances' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantity_Distance_Interaction: 0.5940227020648124\n",
      "Quantity_Squared: 0.21326108274215863\n",
      "Distance_Covered_Squared: 0.12943692727275058\n",
      "log_project_budget: 0.01432131097945496\n",
      "Transaction_Month: 0.011764418408622574\n",
      "supplier_rating: 0.0065221628945606415\n",
      "project_duration: 0.005466251565921377\n",
      "Transaction_Year: 0.004292365452251261\n",
      "transport_mode_1: 0.002864327503181511\n",
      "transport_mode_0: 0.002478693954443479\n",
      "supplier_location_0: 0.002446690729135823\n",
      "project_location_0: 0.0024037122348859203\n",
      "material_category_1: 0.002297656687177717\n",
      "supplier_location_1: 0.002151430825210719\n",
      "project_location_2: 0.002124007750478971\n",
      "material_category_0: 0.0020896908323950144\n",
      "project_location_1: 0.0020565681025581637\n"
     ]
    }
   ],
   "source": [
    "importances = best_rf_model.featureImportances.toArray()\n",
    "\n",
    "feature_names = []\n",
    "\n",
    "# add names for the one-hot encoded categorical features\n",
    "for categoricalCol in categorical_columns:\n",
    "    num_categories = TRAIN_TRANSFORMED_DF.select(categoricalCol + \"Vec\").head()[0].size\n",
    "    feature_names += [f\"{categoricalCol}_{i}\" for i in range(num_categories)]\n",
    "\n",
    "# ddd names for the scaled numerical features\n",
    "feature_names += columns_to_scale\n",
    "\n",
    "# add the log-transformed features if they are also included\n",
    "feature_names.append(\"log_project_budget\")\n",
    "\n",
    "# length of feature names needs to match the length of importances so we are assertign for that\n",
    "assert len(feature_names) == len(importances), f\"Length of feature names ({len(feature_names)}) does not match the number of importances ({len(importances)})\"\n",
    "\n",
    "# match the importances to the feature names\n",
    "named_importances = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# feature importances\n",
    "for name, importance in named_importances:\n",
    "    print(f\"{name}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- quantity_distance_interaction (0.59): Most influential. Indicates combined impact of quantity and distance on predictions\n",
    "- quantity_squared (0.213): shows non-linear relationship of quantity with the target\n",
    "- distance_covered_squared (0.129): shows non-linear effects of distance\n",
    "- log_project_budget (0.014): Modest impact, capturing budget scale effects\n",
    "- transaction_month (0.011): Indicates minor seasonal trends\n",
    "\n",
    "Remaining features have very minimal influence on the model.\n",
    "\n",
    "- given these importances, we can improve the model in future to train on less attributes. For now, I'll carry forward the random forest model to deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# saving my random forest regressor model\n",
    "best_rf_model.write().overwrite().save(RANDOM_FOREST_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_co2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
