{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import isnan, when, count, col, udf, year, month, dayofmonth, dayofweek, datediff, to_date, regexp_replace, length, unix_timestamp, from_unixtime, log\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from constants import TEST_TRANSFORMED_DATA, TRAIN_TRANSFORMED_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CO2 Emission ML Pipeline - Modelling\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#reading df\n",
    "TEST_TRANSFORMED_DF = spark.read.parquet(TEST_TRANSFORMED_DATA)\n",
    "TRAIN_TRANSFORMED_DF = spark.read.parquet(TRAIN_TRANSFORMED_DATA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical columns \n",
    "categorical_columns = [\"transport_mode\", \"project_location\", \"material_category\", \"supplier_location\"]\n",
    "# list of scaled columns\n",
    "columns_to_scale = [\"Quantity_Squared\", \"Distance_Covered_Squared\", \"Quantity_Distance_Interaction\", \"supplier_rating\", \"Transaction_Year\", \"Transaction_Month\", \"project_duration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred                    \n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/home/krisko/ML-pipeline-Co2/venv_co2/lib/python3.10/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.5783348695499914\n",
      "R-squared on test data = 0.7966039182286223\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RandomForest regressor\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"log_CO2_emission\")\n",
    "\n",
    "# Train the model\n",
    "rf_model = rf.fit(TRAIN_TRANSFORMED_DF)\n",
    "\n",
    "# Predict on the test data\n",
    "predictions = rf_model.transform(TEST_TRANSFORMED_DF)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"log_CO2_emission\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")\n",
    "\n",
    "# If you want to evaluate R-squared\n",
    "evaluator = RegressionEvaluator(labelCol=\"log_CO2_emission\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "print(f\"R-squared on test data = {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantity_Distance_Interaction: 0.5573003371925371\n",
      "Quantity_Squared: 0.25001058762081685\n",
      "Distance_Covered_Squared: 0.18706150008736913\n",
      "log_project_budget: 0.0014752473940918102\n",
      "Transaction_Month: 0.0009841259912263885\n",
      "supplier_rating: 0.0007567558169999477\n",
      "transport_mode_1: 0.0004239272269053222\n",
      "Transaction_Year: 0.00034582762793127414\n",
      "project_duration: 0.0002992808920306051\n",
      "material_category_1: 0.00022148226658130296\n",
      "project_location_0: 0.00020682231472337847\n",
      "material_category_0: 0.00020117222781278294\n",
      "project_location_2: 0.00018023740692027566\n",
      "project_location_1: 0.0001580647951555075\n",
      "supplier_location_1: 0.00014353005609024855\n",
      "transport_mode_0: 0.0001341647091086035\n",
      "supplier_location_0: 9.693637369927284e-05\n"
     ]
    }
   ],
   "source": [
    "# Get the feature importances\n",
    "importances = rf_model.featureImportances.toArray()\n",
    "\n",
    "# Start with an empty list for feature names\n",
    "feature_names = []\n",
    "\n",
    "# Add names for the one-hot encoded categorical features\n",
    "# You need to know the number of categories in each categorical feature after one-hot encoding\n",
    "for categoricalCol in categorical_columns:\n",
    "    # Assuming we know the number of categories for each column (replace with actual number)\n",
    "    num_categories = TRAIN_TRANSFORMED_DF.select(categoricalCol + \"Vec\").head()[0].size\n",
    "    feature_names += [f\"{categoricalCol}_{i}\" for i in range(num_categories)]\n",
    "\n",
    "# Add names for the scaled numerical features\n",
    "feature_names += columns_to_scale\n",
    "\n",
    "# Add the log-transformed features if they are also included\n",
    "feature_names.append(\"log_project_budget\")\n",
    "\n",
    "# The length of feature names should now match the length of importances\n",
    "assert len(feature_names) == len(importances), f\"Length of feature names ({len(feature_names)}) does not match the number of importances ({len(importances)})\"\n",
    "\n",
    "# Now you can match the importances to the feature names\n",
    "named_importances = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the feature importances\n",
    "for name, importance in named_importances:\n",
    "    print(f\"{name}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_co2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
